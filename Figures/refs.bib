@techreport{Espeholt,
abstract = {In this work we aim to solve a large collection of tasks using a single reinforcement learning agent with a single set of parameters. A key challenge is to handle the increased amount of data and extended training time. We have developed a new distributed agent IMPALA (Impor-tance Weighted Actor-Learner Architecture) that not only uses resources more efficiently in single-machine training but also scales to thousands of machines without sacrificing data efficiency or resource utilisation. We achieve stable learning at high throughput by combining decoupled acting and learning with a novel off-policy correction method called V-trace. We demonstrate the effectiveness of IMPALA for multi-task reinforcement learning on DMLab-30 (a set of 30 tasks from the DeepMind Lab environment (Beattie et al., 2016)) and Atari-57 (all available Atari games in Arcade Learning Environment (Bellemare et al., 2013a)). Our results show that IMPALA is able to achieve better performance than previous agents with less data, and crucially exhibits positive transfer between tasks as a result of its multi-task approach. The source code is publicly available at github.com/deepmind/scalable agent.},
archivePrefix = {arXiv},
arxivId = {1802.01561v3},
author = {Espeholt, Lasse and Soyer, Hubert and Munos, Remi and Simonyan, Karen and Mnih, Volodymyr and Ward, Tom and Doron, Yotam and Firoiu, Vlad and Harley, Tim and Dunning, Iain and Legg, Shane and Kavukcuoglu, Koray},
eprint = {1802.01561v3},
file = {:C$\backslash$:/Users/ToreH/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Espeholt et al. - Unknown - IMPALA Scalable Distributed Deep-RL with Importance Weighted Actor-Learner Architectures.pdf:pdf},
title = {{IMPALA: Scalable Distributed Deep-RL with Importance Weighted Actor-Learner Architectures}}
}
@misc{OpenAI,
author = {OpenAI},
title = {{Gym}},
url = {https://gym.openai.com/},
urldate = {2020-06-18}
}
@misc{Hilbert2020,
author = {Hilbert, Tore},
title = {{Torehilbert/DeepLearningProject: Repository containing code and jupyter notebook for final project in 02456 Deep Learning at DTU Auther: Tore Hilbert}},
url = {https://github.com/Torehilbert/DeepLearningProject},
urldate = {2020-06-18},
year = {2020}
}
@misc{Scott2004,
author = {Scott, Jeff},
booktitle = {aerospaceweb},
title = {{Aerospaceweb.org | Ask Us - Airfoils at High Angle of Attack}},
url = {http://www.aerospaceweb.org/question/airfoils/q0150b.shtml http://www.aerospaceweb.org/question/aerodynamics/q0165.shtml},
urldate = {2020-05-27},
year = {2004}
}
@techreport{Mnih2016,
abstract = {We propose a conceptually simple and lightweight framework for deep reinforcement learning that uses asynchronous gradient descent for optimization of deep neural network controllers. We present asynchronous variants of four standard reinforcement learning algorithms and show that parallel actor-learners have a stabilizing effect on training allowing all four methods to successfully train neural network controllers. The best performing method, an asynchronous variant of actor-critic, surpasses the current state-of-the-art on the Atari domain while training for half the time on a single multi-core CPU instead of a GPU. Furthermore, we show that asynchronous actor-critic succeeds on a wide variety of continuous motor control problems as well as on a new task of navigating random 3D mazes using a visual input.},
archivePrefix = {arXiv},
arxivId = {1602.01783v2},
author = {Mnih, Volodymyr and {Puigdom{\`{e}}nech Badia}, Adri{\`{a}} and Mirza, Mehdi and Graves, Alex and Harley, Tim and Lillicrap, Timothy P and Silver, David and Kavukcuoglu, Koray},
eprint = {1602.01783v2},
file = {:C$\backslash$:/Users/ToreH/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Mnih et al. - 2016 - Asynchronous Methods for Deep Reinforcement Learning.pdf:pdf},
isbn = {1602.01783v2},
title = {{Asynchronous Methods for Deep Reinforcement Learning}},
year = {2016}
}
